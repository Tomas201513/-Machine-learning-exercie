{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P9fkr2Sh7AYP",
        "_43-nAQpQdcZ",
        "Gzegn3-u_viG",
        "a_sbQtNf_m8t"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization functions ‚õè"
      ],
      "metadata": {
        "id": "P9fkr2Sh7AYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_text(text):\n",
        "    text = text.replace('&amp;', '&')\n",
        "    text = text.replace('&lt;', '<')\n",
        "    text = text.replace('&gt;', '>')\n",
        "    return text\n",
        "    \n",
        "ALLOW_NEW_LINES = False \n",
        "def clean_tweet(tweet, allow_new_lines = ALLOW_NEW_LINES):\n",
        "        bad_start = ['http:', 'https:']\n",
        "        for w in bad_start:\n",
        "            tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
        "            tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
        "            tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
        "            tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
        "            tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
        "        tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space\n",
        "        if not allow_new_lines:                         # TODO: predictions seem better without new lines\n",
        "            tweet = ' '.join(tweet.split())\n",
        "        return tweet.strip()\n",
        "\n",
        "def boring_tweet(tweet):\n",
        "      \"Check if this is a boring tweet\"\n",
        "      boring_stuff = ['http', '@', '#']\n",
        "      not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
        "      return not_boring_words < 3"
      ],
      "metadata": {
        "id": "zN46nli8uFd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset maker üî•"
      ],
      "metadata": {
        "id": "_43-nAQpQdcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json,re,urllib3,random\n",
        "\n",
        "print(\"Enter acount\\'s username without @ sign ‚ùå\\n\")\n",
        "handle=input().lower()\n",
        "\n",
        "cool_tweets = []\n",
        "handles_processed = []\n",
        "raw_tweets = []\n",
        "user_names = []\n",
        "n_tweets_dl = []\n",
        "n_retweets = []\n",
        "n_short_tweets = []\n",
        "n_tweets_kept = []\n",
        "# clear_output(wait=True)\n",
        "http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "res = http.request(\"GET\", f\"http://us-central1-huggingtweets.cloudfunctions.net/get_tweets?handle={handle}&force=1\")\n",
        "res = json.loads(res.data.decode('utf-8'))\n",
        "\n",
        "user_names.append(res['user_name'])\n",
        "all_tweets = res['tweets']\n",
        "raw_tweets.append(all_tweets)\n",
        "curated_tweets = [fix_text(tweet) for tweet in all_tweets]\n",
        "\n",
        "# create dataset\n",
        "clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
        "cool_tweets.append([tweet for tweet in clean_tweets if not boring_tweet(tweet)])\n",
        "\n",
        "# save count\n",
        "n_tweets_dl.append(str(res['n_tweets']))#number of total tweets\n",
        "n_retweets.append(str(res['n_RT']))#number of re-tweets\n",
        "n_short_tweets.append(str(len(all_tweets) - len(cool_tweets[-1]))) #number of short tweets\n",
        "n_tweets_kept.append(str(len(cool_tweets[-1])))#\n",
        "\n",
        "print(f\"\\n{n_tweets_dl[-1]} tweets downloaded, including {n_retweets[-1]} Re-tweets and {n_short_tweets[-1]} short tweets\\nSaving {n_tweets_kept[-1]} tweets in üí•data_{handle}_train.txtüí• \\n\")\n",
        "\n",
        "if len('<|endoftext|>'.join(cool_tweets[-1])) < 6000:\n",
        "  raise ValueError(f\"Error: this user does not have enough tweets to train a Neural Network\\n{res['n_tweets']} tweets downloaded, including {res['n_RT']} RT's and {len(all_tweets) - len(cool_tweets)} boring tweets... only {len(cool_tweets)} tweets kept!\")\n",
        "\n",
        "seed_data = random.randint(0,2**32-1)\n",
        "dataRandom = random.Random(seed_data)\n",
        "total_text = '<|endoftext|>'\n",
        "all_handle_tweets = []\n",
        "epoch_len = max(len(''.join(cool_tweet)) for cool_tweet in cool_tweets)\n",
        "EPOCHS = 4\n",
        "\n",
        "for _ in range(EPOCHS):\n",
        "    for cool_tweet in cool_tweets:\n",
        "        dataRandom.shuffle(cool_tweet)\n",
        "        current_tweet = cool_tweet\n",
        "        current_len = len(''.join(current_tweet))\n",
        "        while current_len < epoch_len:\n",
        "            for t in cool_tweet:\n",
        "                current_tweet.append(t)\n",
        "                current_len += len(t)\n",
        "                if current_len >= epoch_len: break\n",
        "        dataRandom.shuffle(current_tweet)\n",
        "        all_handle_tweets.extend(current_tweet)\n",
        "total_text += '<|endoftext|>'.join(all_handle_tweets) + '<|endoftext|>'\n",
        "\n",
        "with open(f\"data_{handle}_train.txt\", 'w') as f:\n",
        "    f.write(total_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKtunQGyomMN",
        "outputId": "37373529-3a83-4014-f25d-4999299ee93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter acount's username without @ sign ‚ùå\n",
            "\n",
            "BarackObama\n",
            "\n",
            "3250 tweets downloaded, including 330 Re-tweets and 20 short tweets\n",
            "Saving 2900 tweets in üí•data_barackobama_train.txtüí• \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ·ä†·à∞·àç·å•·äï ‚Äçüèãüèæü§∏üèæüö¥üèæüèÉüèæ\n"
      ],
      "metadata": {
        "id": "Gzegn3-u_viG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Have global access\n",
        "trainer = None\n",
        "model_preview, token, namespace = None, None, None\n",
        "\n",
        "# HYPER-PARAMETERS\n",
        "ALLOW_NEW_LINES = False     # seems to work better\n",
        "LEARNING_RATE = 1.372e-4\n",
        "EPOCHS = 4\n",
        "\n",
        "# transformers imports \n",
        "!pip install transformers\n",
        "import transformers,pathlib\n",
        "from transformers import (\n",
        "AutoTokenizer, AutoModelForCausalLM,\n",
        "TextDataset, DataCollatorForLanguageModeling,\n",
        "Trainer, TrainingArguments,\n",
        "get_cosine_schedule_with_warmup)\n",
        "\n",
        "try: \n",
        "# Setting up pre-trained neural network\n",
        "    global trainer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    model = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=pathlib.Path('cache').resolve())\n",
        "    block_size = tokenizer.model_max_length\n",
        "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=f\"data_{handle}_train.txt\", block_size=block_size, overwrite_cache=True)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    seed = random.randint(0,2**32-1)\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=f\"output/{handle}\",\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=0,\n",
        "    seed=seed,\n",
        "    learning_rate = LEARNING_RATE)\n",
        "\n",
        "    # Set-up Trainer\n",
        "    trainer = Trainer(\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset)\n",
        "    \n",
        "    # Update lr scheduler\n",
        "    train_dataloader = trainer.get_train_dataloader()\n",
        "    num_train_steps = len(train_dataloader)\n",
        "    trainer.create_optimizer_and_scheduler(num_train_steps)\n",
        "    trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "        trainer.optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_train_steps)\n",
        "    \n",
        "    #Train\n",
        "    trainer.train()\n",
        "\n",
        "     # set model config parameters\n",
        "    trainer.model.config.task_specific_params['text-generation'] = {\n",
        "        'do_sample': True,\n",
        "        'min_length': 10,\n",
        "        'max_length': 160,\n",
        "        'temperature': 1.,\n",
        "        'top_p': 0.95,\n",
        "        'prefix': '<|endoftext|>'}\n",
        "   \n",
        "    # save new model files\n",
        "    model_name='Tomasmodel'\n",
        "    trainer.save_model(model_name)\n",
        "\n",
        "except Exception as e:\n",
        "                print('\\nAn error occured...\\n')\n",
        "                print(e)\n",
        "                            "
      ],
      "metadata": {
        "id": "jF7XrilK4DVv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dfe43e4-bd89-4b89-d59c-b1f15ad3ddef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /content/cache/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /content/cache/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Creating features from dataset file at \n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (364441 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Saving features into cached file cached_lm_GPT2TokenizerFast_1024_data_barackobama_train.txt [took 0.009 s]\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 355\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 355\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='355' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [355/355 2:10:25, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.554300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.135600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.096300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.062200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.992800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.952300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.882700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.807800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.823500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.602400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.698500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.626200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.597700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.580400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>2.513100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.456300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.539700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.373200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.386600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>2.460300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.461800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.347500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.312900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>2.313200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.382100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>2.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.164400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.302900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>2.275300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.270200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.286700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>2.261700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.295400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>2.237500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>2.205900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.240100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>2.236400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.113800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>2.126600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.100200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>2.233100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.149200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>2.061800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.101100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>2.140600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.151700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>2.126800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.075700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>2.133800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.201200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>2.105300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.133200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>2.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.197900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>2.193400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.181900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>2.156800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.183200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>2.156100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.100100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>2.137300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occured...\n",
            "\n",
            "name 'model_name' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save new model files\n",
        "model_name='Tomasmodel'\n",
        "trainer.save_model(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixT2wszdNA2D",
        "outputId": "85435c23-d0e9-407c-db7b-d04910418f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to Tomasmodel\n",
            "Configuration saved in Tomasmodel/config.json\n",
            "Model weights saved in Tomasmodel/pytorch_model.bin\n",
            "tokenizer config file saved in Tomasmodel/tokenizer_config.json\n",
            "Special tokens file saved in Tomasmodel/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ·â∞·äï·â•·ã≠üéØ\n"
      ],
      "metadata": {
        "id": "a_sbQtNf_m8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start=input()\n",
        "# prepare input\n",
        "start_with_bos = '<|endoftext|>' + start\n",
        "encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
        "\n",
        "# prediction\n",
        "output_sequences = trainer.model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    max_length=160,\n",
        "    min_length=10,\n",
        "    temperature=1.,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=10\n",
        "    )\n",
        "\n",
        "\n",
        "generated_sequences = []\n",
        "\n",
        "# decode prediction\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "    if not ALLOW_NEW_LINES:\n",
        "        limit = text.find('\\n')\n",
        "        text = text[: limit if limit != -1 else None]\n",
        "    generated_sequences.append(text.strip())\n",
        "\n",
        "# print(generated_sequences)\n",
        "print(*generated_sequences, sep = \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdROWwRlXWcA",
        "outputId": "531f79ed-4df0-4016-a0d8-aedb2b691937"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Facebook is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Facebook is the next big step forward in the fight against climate change. Follow the @ObamaFoundation on the progress: #ActOnClimate\n",
            "Facebook is showing how much it‚Äôs been pushing for this election. #TurnaroundDay\n",
            "Facebook is proud to endorse progressive leaders. Join them.\n",
            "Facebook is no longer a partisan issue‚Äîbut a source of great joy to me is when folks like Jackie Robinson and Bruce @Kubrick‚Äôs voices speak out on a basic human right: voting.\n",
            "Facebook is proud to support young people in their communities who have the courage to demand action and say it's time to tackle climate change.\n",
            "Facebook is one of the oldest media platforms. It's where everybody can tune in to watch the President talk about the economy and solve the world's climate crisis. Here is a look at how it helped Americans get organized, organize and #StopGunViolence.\n",
            "Facebook is becoming the internet's favorite social network. And while the digital movement has taken off, the conversation on social media has more than given away. As we look to what the next decade will be about, our shared experience will be one of discovery, reflection, and celebration, and rebirth.\n",
            "Facebook is the best place to stay informed and connect with @allontheline. Join the @OFA Truth Team:\n",
            "Facebook is partnering with FirstStop to deliver new tools to control misinformation around issues:\n",
            "Facebook is proud to announce the first generation of #OFA Fellows:\n"
          ]
        }
      ]
    }
  ]
}