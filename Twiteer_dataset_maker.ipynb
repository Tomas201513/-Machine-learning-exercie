{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P9fkr2Sh7AYP"
      ],
      "authorship_tag": "ABX9TyOZUGHO3LS6R3qzw6SQlrKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomas201513/-Machine-learning-exercie/blob/main/Twiteer_dataset_maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization functions ‚õè"
      ],
      "metadata": {
        "id": "P9fkr2Sh7AYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_text(text):\n",
        "    text = text.replace('&amp;', '&')\n",
        "    text = text.replace('&lt;', '<')\n",
        "    text = text.replace('&gt;', '>')\n",
        "    return text\n",
        "\n",
        "\n",
        "ALLOW_NEW_LINES = False \n",
        "def clean_tweet(tweet, allow_new_lines = ALLOW_NEW_LINES):\n",
        "        bad_start = ['http:', 'https:']\n",
        "        for w in bad_start:\n",
        "            tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
        "            tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
        "            tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
        "            tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
        "            tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
        "        tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space\n",
        "        if not allow_new_lines:                         # TODO: predictions seem better without new lines\n",
        "            tweet = ' '.join(tweet.split())\n",
        "        return tweet.strip()\n",
        "\n",
        "def boring_tweet(tweet):\n",
        "      \"Check if this is a boring tweet\"\n",
        "      boring_stuff = ['http', '@', '#']\n",
        "      not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
        "      return not_boring_words < 3"
      ],
      "metadata": {
        "id": "zN46nli8uFd0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset maker üî•"
      ],
      "metadata": {
        "id": "_43-nAQpQdcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json,re,urllib3,random\n",
        "\n",
        "print(\"Enter acount\\'s username without @ sign ‚ùå\\n\")\n",
        "handle=input().lower()\n",
        "\n",
        "cool_tweets = []\n",
        "handles_processed = []\n",
        "raw_tweets = []\n",
        "user_names = []\n",
        "n_tweets_dl = []\n",
        "n_retweets = []\n",
        "n_short_tweets = []\n",
        "n_tweets_kept = []\n",
        "# clear_output(wait=True)\n",
        "http = urllib3.PoolManager(retries=urllib3.Retry(3))\n",
        "res = http.request(\"GET\", f\"http://us-central1-huggingtweets.cloudfunctions.net/get_tweets?handle={handle}&force=1\")\n",
        "res = json.loads(res.data.decode('utf-8'))\n",
        "\n",
        "user_names.append(res['user_name'])\n",
        "all_tweets = res['tweets']\n",
        "raw_tweets.append(all_tweets)\n",
        "curated_tweets = [fix_text(tweet) for tweet in all_tweets]\n",
        "\n",
        "# create dataset\n",
        "clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
        "cool_tweets.append([tweet for tweet in clean_tweets if not boring_tweet(tweet)])\n",
        "\n",
        "# save count\n",
        "n_tweets_dl.append(str(res['n_tweets']))#number of total tweets\n",
        "n_retweets.append(str(res['n_RT']))#number of re-tweets\n",
        "n_short_tweets.append(str(len(all_tweets) - len(cool_tweets[-1]))) #number of short tweets\n",
        "n_tweets_kept.append(str(len(cool_tweets[-1])))#\n",
        "\n",
        "print(f\"\\n{n_tweets_dl[-1]} tweets downloaded, including {n_retweets[-1]} Re-tweets and {n_short_tweets[-1]} short tweets\\nSaving {n_tweets_kept[-1]} tweets in üí•data_{handle}_train.txtüí• \\n\")\n",
        "\n",
        "if len('<|endoftext|>'.join(cool_tweets[-1])) < 6000:\n",
        "  # need about 4000 chars for one data sample (but depends on spaces, etc)\n",
        "  raise ValueError(f\"Error: this user does not have enough tweets to train a Neural Network\\n{res['n_tweets']} tweets downloaded, including {res['n_RT']} RT's and {len(all_tweets) - len(cool_tweets)} boring tweets... only {len(cool_tweets)} tweets kept!\")\n",
        "\n",
        "seed_data = random.randint(0,2**32-1)\n",
        "dataRandom = random.Random(seed_data)\n",
        "total_text = '<|endoftext|>'\n",
        "all_handle_tweets = []\n",
        "epoch_len = max(len(''.join(cool_tweet)) for cool_tweet in cool_tweets)\n",
        "EPOCHS = 4\n",
        "\n",
        "for _ in range(EPOCHS):\n",
        "    for cool_tweet in cool_tweets:\n",
        "        dataRandom.shuffle(cool_tweet)\n",
        "        current_tweet = cool_tweet\n",
        "        current_len = len(''.join(current_tweet))\n",
        "        while current_len < epoch_len:\n",
        "            for t in cool_tweet:\n",
        "                current_tweet.append(t)\n",
        "                current_len += len(t)\n",
        "                if current_len >= epoch_len: break\n",
        "        dataRandom.shuffle(current_tweet)\n",
        "        all_handle_tweets.extend(current_tweet)\n",
        "total_text += '<|endoftext|>'.join(all_handle_tweets) + '<|endoftext|>'\n",
        "\n",
        "with open(f\"data_{handle}_train.txt\", 'w') as f:\n",
        "    f.write(total_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKtunQGyomMN",
        "outputId": "a4ea1e0e-9be0-4068-8fa0-bff0e8c6e4e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter acount's username without @ sign ‚ùå\n",
            "\n",
            "getfactet\n",
            "\n",
            "289 tweets downloaded, including 8 Re-tweets and 64 short tweets\n",
            "Saving 217 tweets in üí•data_getfactet_train.txtüí• \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jF7XrilK4DVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}